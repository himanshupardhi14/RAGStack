# ğŸ§  Scalable RAG-Based Document Research & Theme Identification Chatbot
RAGStack is an AI-powered platform designed to process, index, and analyze large volumes of documents using Retrieval-Augmented Generation (RAG). It enables intelligent document upload, OCR-based text extraction, semantic chunking, vector embedding, and natural language querying â€” all powered by cutting-edge LLMs (Groq LLM).

Ideal for legal research, academic summarization, enterprise document analysis, and knowledge discovery.

---

##  Features

-  Upload and process 75+ PDF documents with automatic OCR (PyMuPDF / pdfplumber).
-  Chunk, embed, and store documents using vector databases (ChromaDB / FAISS).
-  Ask natural language questions and get answers grounded in document content.
-  Cross-document semantic search using OpenAI/Groq embeddings.
-  Citations returned with `doc_id` and page numbers (e.g., `[doc_1 - Page 4]`).
-  Track uploaded documents and delete specific ones.
-  Fully asynchronous FastAPI backend with modular services.
-  Theme identification and document context summarization (LLM-based).
-  Clean UI (optional React/HTML frontend) with real-time querying.

---

## ğŸ“‚ Project Structure
```bash
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”‚   â”œâ”€â”€ ocr.py
â”‚   â”‚   â”‚   â””â”€â”€ chunking.py
â”‚   â”‚   â””â”€â”€ services/
â”‚   â”‚       â”œâ”€â”€ vectorstore.py
â”‚   â”‚       â””â”€â”€ groq_llm.py
â”‚   â””â”€â”€ data/  â† stores uploaded PDFs
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ script.js
â”‚   â””â”€â”€ static/
â”‚       â”œâ”€â”€ styles.css
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸ§  How It Works

1. **Document Upload**  
   â†’ PDFs are parsed and chunked into small segments.  
   â†’ Each chunk is embedded using an LLM (Groq/OpenAI).  

2. **Storage**  
   â†’ Chunks + metadata are stored in a vector database.

3. **Querying**  
   â†’ User inputs a natural question.  
   â†’ Relevant chunks are retrieved semantically.  
   â†’ Prompt sent to the LLM with context for an answer.  
   â†’ Response includes references like `[doc_id - Page X]`.

---

## âš™ï¸ Tech Stack

| Layer      | Tool/Library                  |
|------------|-------------------------------|
| Backend    | Python, FastAPI, Uvicorn      |
| Embeddings | Groq AI / OpenAI Embeddings   |
| Vector DB  | ChromaDB / FAISS              |
| OCR/PDF    | PyMuPDF / pdfplumber          |
| LLM Prompt | Groq (Mixtral, LLaMA3, etc.)  |
| Frontend   | HTML/CSS/JS or React (Optional) |

---

## âœ… Setup Instructions

1. **Clone the Repo**
```bash
git clone https://github.com/himanshupardhi14/RAGStack.git
cd RAGStack/backend
```
2. **Create a '.env' file in the backend folder and add your API key:**
```bash
GROQ_API_KEY=your_groq_api_key_here
```
3. **Install Dependencies**
```bash
pip install -r requirements.txt
```
4. **Run the FastAPI Server**
```bash
uvicorn app.main:app --reload
```
5. **Open the API Docs**
```bash
Navigate to: http://127.0.0.1:8000/docs
```
---

## ğŸ“ˆ Demo / Use Case
Upload multiple research papers, internal reports, or legal documents â€” then ask:

:-"What are the key themes in Q2 reports?"

:-"Did any document mention GDPR compliance?"

:-"Summarize marketing insights from uploaded PDFs."

---
## Sample Query Flow
```bash
POST /query
Content-Type: application/x-www-form-urlencoded
question=What insights are mentioned in document_3?

Response:
{
  "answer": "Document_3 mentions customer growth insights on [doc_3 - Page 2]."
}
```
---
##  Future Enhancements

 :Frontend UI for seamless upload + querying

 :User authentication & file persistence

 :Summary + theme extraction for dashboards

 :RAG pipelines using LangChain

---
